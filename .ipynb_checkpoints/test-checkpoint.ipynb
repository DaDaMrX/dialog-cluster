{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外部分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "数据清洗开始...\n",
      "Step1: 合并完成 (共10942段对话)\n",
      "Setp2: 正则表达式替换完成\n",
      "Step3: 分词完成\n",
      "Step3: 删除停用词完成 (用时: 20.33s)\n",
      "数据清洗完成.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 1229\n",
      "Time: 21.31999707221985\n",
      "['30', 'app', 'gt', 'id', 'nbsp', 'ok', 'pc', 'plus', 'pos', 'skui', 'source', '一个月', '一些', '一件', '一会', '一会儿', '一共', '一切正常', '一单', '一双', '一台', '一号', '一周', '一块', '一声', '一大', '一天', '一套', '一定', '一家', '一对一', '一年', '一张', '一条', '一次', '一款', '一点', '一直', '一看', '一致', '一起', '七天', '七点', '三个', '三件', '三包', '三天', '三年', '三点', '三级', '上传', '上午', '上帝', '上次', '上海', '上班', '上门', '上面', '下个', '下午', '下发', '下哈', '下次', '下班', '下载', '下面', '不上', '不便', '不到', '不变', '不够', '不太', '不好', '不好意思', '不想', '不换', '不接', '不敢', '不用', '不知', '不能取消', '不行', '不见', '不让', '不足', '不远', '不退', '不送', '不通', '不错', '专业', '专员', '专用发票', '专票', '业务', '东西', '两个', '两件', '两天', '两张']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import logging\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "raw_file = 'chat-short-20w.txt'\n",
    "clean_file = 'chat-short-clean-20w.txt'\n",
    "stop_wrods_file = 'stop_words.txt'\n",
    "\n",
    "jieba.setLogLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "\n",
    "def substitute(sent):\n",
    "    exps = [\n",
    "        r'#E-\\w\\[数字x\\]|~O\\(∩_∩\\)O/~',\n",
    "        r'http[s]?://[a-zA-Z0-9|\\.|/]+',\n",
    "        r'http[s]?://[a-zA-Z0-9\\./-]*\\[链接x\\]',\n",
    "        r'\\[ORDERID_[0-9]+\\]',\n",
    "        r'\\[日期x\\]',\n",
    "        r'\\[时间x\\]',\n",
    "        r'\\[金额x\\]',\n",
    "        r'\\[站点x\\]',\n",
    "        r'\\[数字x\\]',\n",
    "        r'\\[地址x\\]',\n",
    "        r'\\[姓名x\\]',\n",
    "        r'\\[邮箱x\\]',\n",
    "        r'\\[电话x\\]',\n",
    "        r'\\[商品快照\\]',\n",
    "        r'<s>',\n",
    "        r'\\s+',\n",
    "        r'[a-z|0-9]+'\n",
    "        \"[\\s+\\.\\!\\/_,$%^:*(+\\\"\\')]+\",\n",
    "        \"[+——()?:【】‘’“”`！，。？、~@#￥%……&*（）]+\"\n",
    "    ]\n",
    "    for exp in exps:\n",
    "        sent = re.sub(exp, ' ', sent)\n",
    "    return sent\n",
    "\n",
    "logging.info('数据清洗开始...')\n",
    "tt = time.time()\n",
    "\n",
    "# 读取原始数据文件\n",
    "with open(raw_file, encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# 将每段对话合并为一行\n",
    "corpus = corpus.strip().split('\\n\\n')  # '\\n\\n'分隔每段对话\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].strip().split('\\n')  # '\\n'分隔每句话\n",
    "    for j in range(len(corpus[i])):\n",
    "        corpus[i][j] = corpus[i][j].strip()[2:]  # 去除开头的0/1标记\n",
    "    corpus[i] = ' '.join(corpus[i])\n",
    "logging.info('Step1: 合并完成 (共%d段对话)' % len(corpus))\n",
    "\n",
    "\n",
    "# 正则表达式替换特定字符串\n",
    "corpus = list(map(substitute, corpus))\n",
    "logging.info('Setp2: 正则表达式替换完成')\n",
    "\n",
    "# 分词\n",
    "t = time.time()\n",
    "corpus = list(map(jieba.cut, corpus))\n",
    "logging.info('Step3: 分词完成')\n",
    "\n",
    "# 删除停用词\n",
    "with open(stop_wrods_file, encoding='utf-8') as f:\n",
    "    stop_words = f.read().strip().split('\\n')\n",
    "    \n",
    "t = time.time()\n",
    "for i in range(len(corpus)):\n",
    "    tokens = []\n",
    "    for token in corpus[i]:\n",
    "        token = token.strip()\n",
    "        if len(token) > 1 and token not in stop_words:\n",
    "            tokens.append(token)\n",
    "    corpus[i] = tokens\n",
    "logging.info('Step3: 删除停用词完成 (用时: %.2fs)' % (time.time() - t))\n",
    "\n",
    "# Combine\n",
    "corpus = list(map(lambda x: ' '.join(x), corpus))\n",
    "\n",
    "logging.info('数据清洗完成.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_df=0.1\n",
    "min_df = 20\n",
    "max_features=None\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Show data\n",
    "print('words:', len(tfidf_vectorizer.vocabulary_))\n",
    "\n",
    "print('Time:', time.time() - tt)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names()[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
